\documentclass[letter paper, 10 pt, conference]{ieeeconf}


\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

\usepackage[utf8]{inputenc}
% \usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\graphicspath{{arXiv_Figures/}}
\usepackage{color}
\usepackage{ulem}
\usepackage{subcaption}
\usepackage{tikz}
% \usepackage{alphabeta}
\usetikzlibrary{calc,intersections,through,backgrounds, math, angles, quotes}
% \usepackage[]{commath}
% \usepackage[]{enumitem}
\usepackage{tikzscale}
\tikzset{point/.style={circle, fill, inner sep=1.7}}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{cite}


\usepackage{cleveref}
\crefmultiformat{equation}{(#2#1#3)}%
   { and~(#2#1#3)}{, (#2#1#3)}{ and~(#2#1#3)}
\crefrangeformat{equation}{(#3#1#4) to~(#5#2#6)}
\crefformat{equation}{(#2#1#3)}
\crefmultiformat{figure}{Fig.~#2#1#3}%
{ and~Fig.~#2#1#3}{, Fig.~#2#1#3}{ and~Fig.~#2#1#3}
\crefrangeformat{figure}{Figs.~#3#1#4 to~#5#2#6}
\crefformat{figure}{Fig.~#2#1#3}

\renewcommand{\H}{\mathscr{H}}
\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\atan}{atan2}

\usepackage{bm}

\usepackage{enumerate}

%==========================================================

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

% \let\proof\relax
% \let\endproof\relax
% =====================================================

\newcommand{\re}[1]{\textcolor{red}{#1}}
\newcommand{\bl}[1]{\textcolor{blue}{#1}}
\newcommand{\avm}[1]{\textcolor{green!40!black}{[Alex: #1]}}
\newcommand{\ds}[1]{\textcolor{violet}{[Daigo: #1]}}
\newcommand{\dm}[1]{\textcolor{orange!80!black}{[Dipankar: #1]}}
\newcommand{\mike}[1]{\textcolor{olive!80!black}{[Mike: #1]}}
\newcommand{\todo}[1]{\textcolor{red}{[ToDO: #1]}}
% \renewcommand{\baselinestretch}{0.9}
%=========================================================


\renewenvironment{proof}{\textit{Proof:}}{\hfill $\blacksquare$}

\title{\LARGE\bf TITLE }

\author{Aryan Gupta and Dipankar Maity
\thanks{
We gratefully acknowledge the support of ARL grant ARL DCIST CRA W911NF-17-2-0181. The views expressed in this paper are those of the authors and do not reflect the official policy or position of the United States Government, Department of Defense, or its components.
}
\thanks{A. Gupta and D. Maity are with the Department of Electrical and Computer Engineering, University of North Carolina at Charlotte,  NC, 28223, USA.
Email: 		{\{agupta40, dmaity\}@charlotte.edu}
}
}


\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par\vspace{10pt}\hspace{20pt}
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother


\begin{document}

\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}


\begin{abstract}

\end{abstract}

\section{Introduction}


\section{Problem Formulation}
In typical motion planning problems, a task or mission and an environment is given. The agent then must satisfy the task while minimizing the movement in the environment. In more difficult agent assignments, risk is added to the environment that prevent the agent from moving into certain areas. To add another layer of complexity, the risk can be unknown or change over time; similarly, the task can change without the knowledge of the agent.

In previous work, in order to satisfy the task while maintaining a maximal level of risk aversion while minimizing the distance traveled in the environment, a cross product of the environment and task is used. First, the environment is discretized into cells, then the cells are used to construct a graph where the individual cells are the nodes in the graph and the possible movement into the surrounding cells are the edges of the graph. In this paper, only 4 edges are considered: moving up, down, left, and right; moving diagonally is not considered, and is outside of the scope of this paper. Next, the task is also converted into a graph. For this, the task is inputted as an LTL equation, this allows a modular way to assign tasks to the agent and also allows an easy method of converting this task into a graph, typically a Buchii Automata. Thirdly, once the environment and task are converted into a graph, a product of those two graphs are taken. This creates a larger graph of all possible states that the agent can be in and move into, in the combined context of the environment and the task. Finally, a path finding algorithm is used such as Dijkstra's or A* to create a path from the start node to the accepting state of the task.

Issues start to arise when unknown risks, unknown environment, and unknown tasks are introduced. If any of the risk areas or any part of the environment changes, the weights of the edges and the respective cells must change, typically to avoid a large computational overhead, the entire process is repeated. The changed environment or risk must be rediscretized and the product graph recalculated. A similar problem arises when the task changes.

In this paper, an alternative method will be discussed that sacrifices some determinicy for a lower computation overhead. By calculating some heuristical data, the agent can calculate the product automata, on the fly without the need to calculate it in full. This has the additional value if large parts of the environment or task space will not be traversed through.

% In a typical pathfinding situation, an agent uses a popular technique that involves cellifying an environment into a graph, creating a task graph of the mission at hand, createing a product automata of these two graphs, and finally applying Dijkstra's algorithm on the final graph. This is highly deterministic and will guarentee the agent will find the shortest path that will complete the mission and travel the shortest path in the environment. However, issues arise when the environment and the task are dynamic. The agent must recreate the environment graph each time the environment changes, then recompute the product automata and then rerun the Dijkstra's algoirithm. If the task changes, the agent must convert the mission into a graph and recompute the product automata. In this paper, we peopose an alternative algorithm that sacrifices some determinicy for speed and computational efficiency.

\section{Simulation Results}
In this paper, six experiments will be done to exemplify the novel method. Each example will use this environment as the base and change the environment in order to exemplify the exact problem being solved. The command used for the examples are shown below. This command example will be moified for each example.

\begin{verbatim}
$ cd fast-risk-aware-ltl-planning/code
$ export PYTHON='python3'
$ export EXE='main.py'
$ export ARGS='
  --seed 1
  --cell-size 32
  --view-cell-size 32
  --height 160
  --width 160
  --env "../maps/normal-example.bmp"
  --no-assumed-risk
  --task "../tasks/basic-ab.hoa.txt"
  --output "../../../tmp"
'
$ exec $PYTHON $EXE $ARGS
\end{verbatim}

\subsection{Experiment 1}
In this experiment we consider a simple 5 cell by 5 cell environment with 4 "targets" and 4 hazard cells as seen in Figure \ref{fig:env1}. Further experiments will expand on this basic example with small changes to exemplify the various experiments. The LTL equation is also very simple. The agent starts at A is required to visit one of the cells (B or C) before finishing the mission (D cell). Reference equation \ref{equ:basic-ab} and Figure \ref{fig:buchii-basic-ab} for the resultant Buchii automata.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figs/normal-example.png}
  \caption{Environment for Experiment 1}
  \label{fig:env1}
\end{figure}

\begin{equation}
  Fz \& (!z U c) \& (!z U b)
  \label{equ:basic-ab}
\end{equation}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figs/basic-ab-ltl-buchii-full.png}
  \caption{Full Buchii Automata for Equation \ref{equ:basic-ab}}
  \label{fig:buchii-basic-ab}
\end{figure}

As seen in the buchii automata, if the agent compleates the B target first, it will be in state 2 of the buchii automata. If the agent compleates the C task first then it will be in state 4. Then the states combine into state 1 so the agent may compleate task Z. Figure \ref{fig:exp1-final-path} shows the path the agent took in light blue. As seen, the agent chooses task C first, then task B then task Z. The agent also could have taken task B first, C second, and lastly task Z. The reason it chose one over the other is random since they both have the same cell length path. In further experiments, a asynconosous shorter path will be created to see if the agent chooses the shorter of the two paths.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figs/exp1-final-path.png}
  \caption{Environment for Experiment 1}
  \label{fig:exp1-final-path}
\end{figure}

\subsection{Experiment 2 \& 3}
In experiment 2 and 3 we move the D cell closer to cell one of the middle cells (B or C), this creates a path that is more favorable. Figure \ref{fig:env2-choib} shows the environment that favors the B path and Figure \ref{fig:env3-choic} showd the environment that favors the C path. The LTL equation will remain the same as Equation \ref{equ:basic-ab}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figs/choib-example.png}
  \caption{Environment for Experiment 2 -- Choose B}
  \label{fig:env2-choib}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{figs/choic-example.png}
  \caption{Environment for Experiment 2 -- Choose B}
  \label{fig:env2-choib}
\end{figure}


As you can see the agent chooses the shorter path depending on the shorter path.


In experiment 4, we revert to the original environment but change the LTL equation so the agent must visit both B and C cells before visiting the D cell. This example is rudamentry and will be used to build to the next example

In experiment 5, we change the environment so the C cell is closer to the A cell. This creates a asynconosous path. The A -> C -> B -> D path is shorter than the A -> B -> C -> D and so the agent chooses the former

For experiment 6, we introduce task switching.


In this experiment we consider the environment shown in \ref{fig:env1}.
The environment consists of 9 cells. The corner cells contain targets and the center cell is a hazard.
% minimal viable example:
% [IMAGE CREATED] 5x5 or 3x3 grid with one or two cells that are the targets
% the center of the grid and maybe few other cells are hazards
% LTL task: `Fz & (!z U c) & (!z U b)`
% LTL eng task: go to c and b in any order then go to z
We consider the LTL task ``insert LTL Fomula here'' which specifies that the robot performs ``insert english language description of the task here" .
The solution obtained from our algorithm is shown by the magenta path of the robot.
** What did we learn/observe from this experiment:**
% the algorithm can create a basic environment graph and optimally pathfind from the start to the end
How good is your solution compared to the optimal one if we implemented the product automata framework?
% INSERT graph of timing here
How long did it take your framework to obtain the solution?
% discuss timing results
Which part of the code was most time consuming?
How much computationally efficient is your code  (e.g., memory/time requirements) compared to the product automata approach?
% \begin{figure}
%     \centering
%     \includegraphics[scale = .2, width = 0.9 \linewidth]{figs/hospital-cells.png}
%     \caption{Environment for Experiment-1.}
%     \label{fig:env1}
% \end{figure}

\bibliographystyle{ieeetr}
% \bibliography{references}

\end{document}
